#!/usr/bin/env python3
"""
OPTIMIZED AI EXTRACTION SYSTEM
Two-step process: Extract â†’ Validate
Trains AI based on data schema and rules for document data extraction
"""

import os
import json
import logging
import base64
import re
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ValidationStatus(Enum):
    VALID = "valid"
    WARNING = "warning" 
    INVALID = "invalid"

class FieldType(Enum):
    TEXT = "TEXT"
    NUMBER = "NUMBER"
    DATE = "DATE"
    BOOLEAN = "BOOLEAN"

@dataclass
class ExtractionResult:
    """Result of document extraction process"""
    success: bool
    extracted_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    documents_processed: int = 0

@dataclass
class ValidationResult:
    """Result of field validation process"""
    success: bool
    updated_validations: Optional[List[Dict[str, Any]]] = None
    error_message: Optional[str] = None
    validations_processed: int = 0

class DocumentProcessor:
    """Handles document text extraction from various formats"""
    
    @staticmethod
    def extract_text_content(doc: Dict[str, Any]) -> str:
        """Extract text from document based on MIME type"""
        file_content = doc.get('file_content', '')
        file_name = doc.get('file_name', 'unknown')
        mime_type = doc.get('mime_type', 'text/plain')
        
        try:
            if mime_type.startswith("text/"):
                return DocumentProcessor._extract_text(file_content)
            else:
                # For binary files, content will be processed by Gemini API
                return f"[BINARY FILE: {file_name}]"
                
        except Exception as e:
            logger.error(f"Failed to extract text from {file_name}: {e}")
            return f"[ERROR EXTRACTING: {file_name}]"
    
    @staticmethod
    def _extract_text(content: Any) -> str:
        """Extract text from various content formats"""
        if isinstance(content, str):
            if content.startswith('data:'):
                # Handle data URL
                try:
                    base64_content = content.split(',', 1)[1]
                    decoded_bytes = base64.b64decode(base64_content)
                    return decoded_bytes.decode('utf-8', errors='ignore')
                except Exception as e:
                    logger.error(f"Failed to decode data URL: {e}")
                    return ""
            return content
        elif isinstance(content, bytes):
            return content.decode('utf-8', errors='ignore')
        else:
            return str(content)

class PromptBuilder:
    """Builds AI prompts for extraction and validation"""
    
    def __init__(self):
        self.logger = logger
    
    def build_extraction_prompt(
        self,
        documents: List[Dict[str, Any]],
        project_schema: Dict[str, Any],
        extraction_rules: List[Dict[str, Any]],
        session_name: str
    ) -> str:
        """Build comprehensive extraction prompt"""
        
        doc_names = [doc.get('file_name', 'Unknown') for doc in documents]
        
        prompt = f"""You are an expert data extraction specialist. Extract data from {len(documents)} documents and return structured JSON.

CRITICAL INSTRUCTIONS:
1. PROCESS ALL DOCUMENTS: {doc_names}
2. FOLLOW SCHEMA DESCRIPTIONS: Each description is your extraction instruction
3. APPLY EXTRACTION RULES: Rules modify behavior, formatting, and validation
4. COUNT COMPREHENSIVELY: For NUMBER fields, count ALL instances across ALL documents
5. EXTRACT ALL COLLECTIONS: Find EVERY instance mentioned across documents
6. RETURN REAL VALUES ONLY: No placeholders or examples

DOCUMENT SET: Processing {len(documents)} documents simultaneously.

EXTRACTION SCHEMA:"""
        
        # Add schema fields
        schema_fields = project_schema.get("schema_fields", [])
        if schema_fields:
            prompt += "\nPROJECT FIELDS:"
            for field in schema_fields:
                field_info = self._format_field_info(field, extraction_rules)
                prompt += f"\n- {field_info}"
        
        # Add collections
        collections = project_schema.get("collections", [])
        if collections:
            prompt += "\n\nCOLLECTIONS (extract ALL instances):"
            for collection in collections:
                collection_info = self._format_collection_info(collection, extraction_rules)
                prompt += f"\n{collection_info}"
        
        # Add dynamic JSON example
        json_example = self._generate_json_example(project_schema, session_name, extraction_rules)
        prompt += f"\n\nEXPECTED OUTPUT FORMAT:\n{json_example}"
        
        prompt += f"""

VERIFICATION CHECKLIST:
- Processed all {len(documents)} documents: {doc_names}
- Applied all extraction rules where applicable
- Counted all instances for NUMBER fields
- Extracted all collection items found

RETURN ONLY VALID JSON - NO EXPLANATIONS"""
        
        return prompt
    
    def build_validation_prompt(
        self,
        field_validations: List[Dict[str, Any]],
        extraction_rules: List[Dict[str, Any]],
        knowledge_documents: List[Dict[str, Any]]
    ) -> str:
        """Build validation prompt for field records"""
        
        prompt = f"""You are an expert data validation specialist. Review {len(field_validations)} field records and apply validation rules.

VALIDATION INSTRUCTIONS:
1. APPLY EXTRACTION RULES: Use rules to adjust confidence and format values
2. CONFIDENCE ADJUSTMENT: If rule specifies percentage (e.g., "27%"), use that confidence
3. KNOWLEDGE CONFLICTS: Lower confidence when values conflict with knowledge base
4. FORMAT TRANSFORMATION: Apply formatting rules to values
5. PRESERVE UUIDS: Use exact UUIDs from input
6. PROVIDE REASONING: Explain confidence adjustments and rule applications

FIELD RECORDS TO VALIDATE:"""
        
        # Add field validation records
        for fv in field_validations:
            uuid = fv.get('uuid', fv.get('id', 'unknown'))
            field_name = fv.get('field_name', fv.get('fieldName', 'unknown'))
            field_value = fv.get('extracted_value', fv.get('fieldValue'))
            field_type = fv.get('field_type', fv.get('fieldType', 'TEXT'))
            
            prompt += f"\n- UUID: {uuid} | Field: {field_name} ({field_type}) | Value: {field_value}"
        
        # Add extraction rules
        if extraction_rules:
            prompt += "\n\nEXTRACTION RULES:"
            for rule in extraction_rules:
                if rule.get('isActive', True):
                    rule_info = self._format_rule_info(rule)
                    prompt += f"\n- {rule_info}"
        
        # Add knowledge context
        if knowledge_documents:
            prompt += "\n\nKNOWLEDGE BASE:"
            for doc in knowledge_documents:
                doc_name = doc.get('displayName', doc.get('fileName', 'Document'))
                content = doc.get('content', '')[:500]  # Limit content length
                if content:
                    prompt += f"\n- {doc_name}: {content}..."
        
        # Add output format
        prompt += """

OUTPUT FORMAT:
{
  "fieldValidations": [
    {
      "uuid": "exact-uuid-from-input",
      "fieldName": "fieldName",
      "fieldType": "fieldType",
      "fieldValue": "value-after-rule-formatting",
      "collectionID": "collectionId-or-null",
      "validationStatus": "valid|warning|invalid",
      "validationConfidence": 0.95,
      "AIReasoning": "Detailed reasoning for confidence and status"
    }
  ]
}

RETURN ONLY VALID JSON - NO EXPLANATIONS"""
        
        return prompt
    
    def _format_field_info(self, field: Dict[str, Any], rules: List[Dict[str, Any]]) -> str:
        """Format field information with applicable rules"""
        field_name = field.get('fieldName', '')
        field_type = field.get('fieldType', 'TEXT')
        description = field.get('description', 'Extract this field')
        camel_case_name = field_name.replace(' ', '').replace('of', 'Of')
        
        # Find applicable rules
        applicable_rules = self._find_applicable_rules(field_name, rules)
        
        info = f"**{camel_case_name}** ({field_type}): {description}"
        if applicable_rules:
            rule_text = " | ".join([f"RULE: {rule.get('ruleContent', '')}" for rule in applicable_rules])
            info += f" | {rule_text}"
        
        return info
    
    def _format_collection_info(self, collection: Dict[str, Any], rules: List[Dict[str, Any]]) -> str:
        """Format collection information with properties and rules"""
        collection_name = collection.get('collectionName', collection.get('objectName', ''))
        description = collection.get('description', 'Extract array of objects')
        
        # Find applicable rules for collection
        applicable_rules = self._find_applicable_rules(collection_name, rules)
        
        info = f"- **{collection_name}**: {description}"
        if applicable_rules:
            rule_text = " | ".join([f"RULE: {rule.get('ruleContent', '')}" for rule in applicable_rules])
            info += f" | {rule_text}"
        
        # Add properties
        properties = collection.get("properties", [])
        if properties:
            info += f"\n  Properties for each {collection_name}:"
            for prop in properties:
                prop_info = self._format_property_info(prop, collection_name, rules)
                info += f"\n  * {prop_info}"
        
        return info
    
    def _format_property_info(self, prop: Dict[str, Any], collection_name: str, rules: List[Dict[str, Any]]) -> str:
        """Format property information with applicable rules"""
        prop_name = prop.get('propertyName', '')
        prop_type = prop.get('propertyType', 'TEXT')
        description = prop.get('description', 'Extract this property')
        
        # Find applicable rules for this property
        full_prop_name = f"{collection_name}.{prop_name}"
        arrow_notation = f"{collection_name} --> {prop_name}"
        
        applicable_rules = []
        for rule in rules:
            rule_target = rule.get('targetField', [])
            if isinstance(rule_target, list):
                if any(target in [arrow_notation, full_prop_name, prop_name, 'All Fields'] 
                       for target in rule_target):
                    applicable_rules.append(rule)
            elif rule_target in [arrow_notation, full_prop_name, prop_name, 'All Fields']:
                applicable_rules.append(rule)
        
        info = f"**{prop_name}** ({prop_type}): {description}"
        if applicable_rules:
            rule_text = " | ".join([f"RULE: {rule.get('ruleContent', '')}" for rule in applicable_rules])
            info += f" | {rule_text}"
        
        return info
    
    def _format_rule_info(self, rule: Dict[str, Any]) -> str:
        """Format rule information for validation"""
        rule_name = rule.get('ruleName', 'Unknown Rule')
        rule_content = rule.get('ruleContent', '')
        target_field = rule.get('targetField', '')
        
        info = f"**{rule_name}**: {rule_content}"
        
        # Show target fields
        if isinstance(target_field, list):
            info += f" (applies to: {', '.join(target_field)})"
        else:
            info += f" (applies to: {target_field})"
        
        # Extract confidence percentage
        confidence_match = re.search(r'(\d{1,2})%', rule_content)
        if confidence_match:
            confidence_pct = confidence_match.group(1)
            info += f" [CONFIDENCE: 0.{confidence_pct.zfill(2)}]"
        
        return info
    
    def _find_applicable_rules(self, field_name: str, rules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find rules applicable to a specific field"""
        applicable_rules = []
        for rule in rules:
            rule_target = rule.get('targetField', [])
            if isinstance(rule_target, list):
                if field_name in rule_target or 'All Fields' in rule_target:
                    applicable_rules.append(rule)
            elif field_name == rule_target or rule_target == 'All Fields':
                applicable_rules.append(rule)
        return applicable_rules
    
    def _generate_json_example(self, schema: Dict[str, Any], session_name: str, rules: List[Dict[str, Any]]) -> str:
        """Generate dynamic JSON example based on schema"""
        json_lines = [f'{{', f'  "{session_name}": {{']
        
        # Add schema fields
        schema_fields = schema.get("schema_fields", [])
        for field in schema_fields:
            field_name = field.get('fieldName', '')
            field_type = field.get('fieldType', 'TEXT')
            camel_case_name = field_name.replace(' ', '').replace('of', 'Of')
            
            # Determine example value based on type
            if field_type == FieldType.NUMBER.value:
                example_value = '42'
            elif field_type == FieldType.DATE.value:
                example_value = '"2024-01-15"'
            elif field_type == FieldType.BOOLEAN.value:
                example_value = 'true'
            else:  # TEXT
                example_value = '"Extracted Value"'
            
            json_lines.append(f'    "{camel_case_name}": {example_value},')
        
        # Add collections
        collections = schema.get("collections", [])
        for collection in collections:
            collection_name = collection.get('collectionName', collection.get('objectName', ''))
            json_lines.append(f'    "{collection_name}": [')
            json_lines.append('      {')
            
            properties = collection.get("properties", [])
            for i, prop in enumerate(properties):
                prop_name = prop.get('propertyName', '')
                prop_type = prop.get('propertyType', 'TEXT')
                
                # Determine example value
                if prop_type == FieldType.NUMBER.value:
                    example_value = '100'
                elif prop_type == FieldType.DATE.value:
                    example_value = '"2024-01-15"'
                elif prop_type == FieldType.BOOLEAN.value:
                    example_value = 'true'
                else:  # TEXT
                    example_value = '"Real Value"'
                
                comma = ',' if i < len(properties) - 1 else ''
                json_lines.append(f'        "{prop_name}": {example_value}{comma}')
            
            json_lines.append('      }')
            json_lines.append('    ],')
        
        json_lines.extend(['  }', '}'])
        return '\n'.join(json_lines)

class AIExtractionService:
    """Main service for AI-powered document extraction and validation"""
    
    def __init__(self):
        self.prompt_builder = PromptBuilder()
        self.document_processor = DocumentProcessor()
        self.logger = logger
    
    def _get_gemini_model(self):
        """Initialize and return Gemini model"""
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable not found")
        
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            return genai.GenerativeModel('gemini-1.5-flash')
        except ImportError:
            raise ImportError("google-generativeai package not installed. Install with: pip install google-generativeai")
    
    def extract_from_documents(
        self,
        documents: List[Dict[str, Any]], 
        project_schema: Dict[str, Any],
        extraction_rules: List[Dict[str, Any]] = None,
        session_name: str = "contract"
    ) -> ExtractionResult:
        """
        STEP 1: Extract structured data from documents using AI
        
        Args:
            documents: List of document objects with file_content, file_name, mime_type
            project_schema: Schema with schema_fields and collections
            extraction_rules: Optional rules for extraction guidance
            session_name: Name for the main object container
            
        Returns:
            ExtractionResult with success status and extracted data
        """
        try:
            if not documents:
                return ExtractionResult(success=False, error_message="No documents provided")
            
            self.logger.info(f"Starting extraction for {len(documents)} documents")
            
            model = self._get_gemini_model()
            extraction_rules = extraction_rules or []
            
            # Build extraction prompt
            prompt = self.prompt_builder.build_extraction_prompt(
                documents, project_schema, extraction_rules, session_name
            )
            
            # Prepare content for Gemini API
            content_parts = [prompt]
            file_parts = []
            
            # Process documents
            for doc in documents:
                file_content = doc.get('file_content', '')
                file_name = doc.get('file_name', 'unknown')
                mime_type = doc.get('mime_type', 'text/plain')
                
                if mime_type.startswith("text/"):
                    # Add text content to prompt
                    text_content = self.document_processor.extract_text_content(doc)
                    content_parts.append(f"\n\n=== DOCUMENT: {file_name} ===\n{text_content}")
                else:
                    # Prepare binary file for Gemini
                    try:
                        if isinstance(file_content, str) and file_content.startswith('data:'):
                            mime_part, base64_content = file_content.split(',', 1)
                            binary_content = base64.b64decode(base64_content)
                            
                            file_part = {
                                "mime_type": mime_type,
                                "data": binary_content
                            }
                            content_parts.append(file_part)
                            file_parts.append(file_name)
                            
                    except Exception as e:
                        self.logger.error(f"Failed to process {file_name}: {e}")
                        continue
            
            # Make AI extraction call
            self.logger.info(f"Processing documents: {[doc.get('file_name') for doc in documents]}")
            if file_parts:
                self.logger.info(f"Binary files processed by Gemini: {file_parts}")
                
            response = model.generate_content(content_parts)
            
            if not response or not response.text:
                return ExtractionResult(success=False, error_message="No response from AI")
            
            # Parse and clean response
            extracted_data = self._parse_json_response(response.text, session_name)
            
            if extracted_data is None:
                return ExtractionResult(success=False, error_message="Failed to parse AI response")
            
            self.logger.info(f"Successfully extracted data with keys: {list(extracted_data.keys())}")
            return ExtractionResult(
                success=True, 
                extracted_data=extracted_data,
                documents_processed=len(documents)
            )
            
        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")
            return ExtractionResult(success=False, error_message=str(e))
    
    def validate_field_records(
        self,
        field_validations: List[Dict[str, Any]],
        extraction_rules: List[Dict[str, Any]] = None,
        knowledge_documents: List[Dict[str, Any]] = None
    ) -> ValidationResult:
        """
        STEP 2: Validate field records using AI and rules
        
        Args:
            field_validations: Field validation records with UUIDs
            extraction_rules: Rules for validation guidance
            knowledge_documents: Knowledge base for context
            
        Returns:
            ValidationResult with updated validation records
        """
        try:
            if not field_validations:
                return ValidationResult(success=False, error_message="No field validations provided")
            
            self.logger.info(f"Starting validation for {len(field_validations)} field records")
            
            model = self._get_gemini_model()
            extraction_rules = extraction_rules or []
            knowledge_documents = knowledge_documents or []
            
            # Build validation prompt
            prompt = self.prompt_builder.build_validation_prompt(
                field_validations, extraction_rules, knowledge_documents
            )
            
            # Make AI validation call
            response = model.generate_content(prompt)
            
            if not response or not response.text:
                return ValidationResult(success=False, error_message="No response from AI")
            
            # Parse validation response
            validation_data = self._parse_json_response(response.text)
            
            if validation_data is None:
                return ValidationResult(success=False, error_message="Failed to parse validation response")
            
            updated_validations = validation_data.get('fieldValidations', [])
            
            self.logger.info(f"Successfully validated {len(updated_validations)} field records")
            return ValidationResult(
                success=True,
                updated_validations=updated_validations,
                validations_processed=len(updated_validations)
            )
            
        except Exception as e:
            self.logger.error(f"Validation failed: {e}")
            return ValidationResult(success=False, error_message=str(e))
    
    def _parse_json_response(self, response_text: str, session_name: str = None) -> Optional[Dict[str, Any]]:
        """Parse and clean JSON response from AI"""
        try:
            # Clean response text
            cleaned_text = response_text.strip()
            
            # Remove markdown code blocks
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:]
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]
            
            cleaned_text = cleaned_text.strip()
            
            # Handle empty response
            if not cleaned_text:
                if session_name:
                    return {session_name: {}}
                return {}
            
            # Parse JSON
            return json.loads(cleaned_text)
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON parsing failed: {e}")
            self.logger.error(f"Cleaned response: {cleaned_text[:500]}...")
            return None

# Main execution functions
def step1_extract_from_documents(
    documents: List[Dict[str, Any]], 
    project_schema: Dict[str, Any],
    extraction_rules: List[Dict[str, Any]] = None,
    session_name: str = "contract"
) -> ExtractionResult:
    """Convenience function for document extraction"""
    service = AIExtractionService()
    return service.extract_from_documents(documents, project_schema, extraction_rules, session_name)

def step2_validate_field_records(
    field_validations: List[Dict[str, Any]],
    extraction_rules: List[Dict[str, Any]] = None,
    knowledge_documents: List[Dict[str, Any]] = None
) -> ValidationResult:
    """Convenience function for field validation"""
    service = AIExtractionService()
    return service.validate_field_records(field_validations, extraction_rules, knowledge_documents)

def extract_and_validate_chain(
    documents: List[Dict[str, Any]], 
    project_schema: Dict[str, Any],
    extraction_rules: List[Dict[str, Any]] = None,
    knowledge_documents: List[Dict[str, Any]] = None,
    session_name: str = "contract"
) -> Tuple[ExtractionResult, Optional[ValidationResult]]:
    """
    Execute both extraction and validation steps in sequence
    
    Returns:
        Tuple of (extraction_result, validation_result)
    """
    logger.info("Starting chained extraction and validation process")
    
    service = AIExtractionService()
    
    # Step 1: Extract
    extraction_result = service.extract_from_documents(
        documents, project_schema, extraction_rules, session_name
    )
    
    if not extraction_result.success:
        return extraction_result, None
    
    # Note: Step 2 would typically be called by the API layer after
    # creating field validation records from the extraction results
    
    return extraction_result, None

if __name__ == "__main__":
    import sys
    
    try:
        # Read input from stdin
        input_data = json.loads(sys.stdin.read())
        step = input_data.get("step", "extract")
        
        if step == "extract":
            result = step1_extract_from_documents(
                documents=input_data.get("files", []),
                project_schema=input_data.get("project_schema", {}),
                extraction_rules=input_data.get("extraction_rules", []),
                session_name=input_data.get("session_name", "contract")
            )
            
            if result.success:
                print(json.dumps(result.extracted_data))
            else:
                print(json.dumps({"error": result.error_message}), file=sys.stderr)
                sys.exit(1)
                
        elif step == "validate":
            result = step2_validate_field_records(
                field_validations=input_data.get("field_validations", []),
                extraction_rules=input_data.get("extraction_rules", []),
                knowledge_documents=input_data.get("knowledge_documents", [])
            )
            
            if result.success:
                print(json.dumps({"fieldValidations": result.updated_validations}))
            else:
                print(json.dumps({"error": result.error_message}), file=sys.stderr)
                sys.exit(1)
                
        else:
            print(json.dumps({"error": f"Unknown step: {step}"}), file=sys.stderr)
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"Execution failed: {e}")
        print(json.dumps({"error": str(e)}), file=sys.stderr)
        sys.exit(1)